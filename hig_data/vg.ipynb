{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload \n",
    "%autoreload 2\n",
    "from hig_data.vg2 import VGGraphPrecomputedDataset\n",
    "\n",
    "# path,                       # Path to the precomputed h5 file \n",
    "#                  vocab_path,                 # Path to the vocab h5 file\n",
    "#                  split_path,                 # Path to dataset ID splits\n",
    "\n",
    "vg = VGGraphPrecomputedDataset(path='/home/rfsm2/rds/hpc-work/datasets/vg/all_vg_graph.h5',\n",
    "                         vocab_path='/home/rfsm2/rds/hpc-work/datasets/vg/all_vg_clip_vocab.h5',\n",
    "                        split_path='/home/rfsm2/rds/hpc-work/datasets/vg/raw_vg/vg_splits.json',)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "with h5py.File(vg.path, 'r') as hdf:\n",
    "        group = hdf[str(vg._data_fnames[1])]\n",
    "        print(list(group.keys()))\n",
    "        image = group['image'][:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hig_data.visualisation import plot_array_images\n",
    "from training.encoders import StabilityVAEEncoder\n",
    "import numpy as np\n",
    "import torch\n",
    "vae = StabilityVAEEncoder(batch_size=8, vae_name='stabilityai/sdxl-vae')\n",
    "ground_truth = vae.decode(vae.encode_latents(torch.tensor(image[np.newaxis,...]).to('cuda')))\n",
    "plot_array_images(ground_truth.cpu()) \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile\n",
    "\n",
    "latents = zipfile.ZipFile('/home/rfsm2/rds/hpc-work/datasets/vg/all_vg_512-sd.zip')\n",
    "files = latents.filelist\n",
    "with latents.open(files[0], 'r') as f:\n",
    "    img = np.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hig_data.visualisation import plot_array_images\n",
    "from training.encoders import StabilityVAEEncoder\n",
    "import numpy as np\n",
    "import torch\n",
    "vae = StabilityVAEEncoder(batch_size=8, vae_name='stabilityai/sdxl-vae')\n",
    "ground_truth = vae.decode(vae.encode_latents(torch.tensor(img[np.newaxis,...]).to('cuda')))\n",
    "plot_array_images(ground_truth.cpu()) \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vg[0].metadata()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from training.networks_edm2_hignn_control import Precond\n",
    "import torch\n",
    "precond = Precond(64, 4, gnn_metadata = vg[0].metadata()).cuda()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# precond\n",
    "blank_image = torch.zeros((1, 192, 64, 64)).cuda()\n",
    "out, new_graph = precond.unet.hignn(blank_image, vg[2].clone().cuda())\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hig_data.visualisation import plot_array_images\n",
    "plot_array_images(out.detach().cpu()[:,3:6]) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hig_data.utils import DataLoader\n",
    "dls = DataLoader(vg, batch_size=8, subsample=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_batch = next(iter(dls))\n",
    "graph_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hig_data.visualisation import plot_array_images\n",
    "from training.encoders import StabilityVAEEncoder\n",
    "vae = StabilityVAEEncoder(batch_size=8,)\n",
    "ground_truth = vae.decode(vae.encode_latents(graph_batch.image.to('cuda')))\n",
    "plot_array_images(ground_truth.cpu()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload \n",
    "%autoreload 2\n",
    "from hig_data.vg import VgSceneGraphDataset\n",
    "\n",
    "root = '/home/rfsm2/rds/hpc-work/datasets/vg/raw_vg/'\n",
    "vg = VgSceneGraphDataset(vocab_json=root+'vocab.json',\n",
    "                         h5_path=root+'val.h5',\n",
    "                        #  image_dir='/home/rfsm2/rds/hpc-work/datasets/vg/all_vg_512',\n",
    "                        image_dir=root+'images',\n",
    "                         image_size=512,)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "path = '/home/rfsm2/rds/hpc-work/datasets/vg/all_vg_clip_vocab.h5'\n",
    "with h5py.File(path, 'w') as hdf:\n",
    "    vocab = hdf.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = 650\n",
    "objs, obj_bbox, triples, img_path, img = vg[index]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print([vg.vocab['object_idx_to_name'][o] for o in objs])\n",
    "print(vg.data['attributes_per_object'][index], vg.data['object_attributes'][index])\n",
    "# print(vg.data['object_attributes'][index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "import numpy as np\n",
    "def visualize_bounding_boxes(image: np.ndarray, bboxes: np.ndarray, labels: list, bbox_color='red', text_color='white'):\n",
    "    \"\"\"\n",
    "    Visualizes bounding boxes on a numpy array image.\n",
    "    \n",
    "    Arguments:\n",
    "        image: 2D NumPy array representing the image.\n",
    "        bboxes: Bounding boxes as an Nx4 numpy array where each row is [xmin, ymin, xmax, ymax].\n",
    "                Coordinates are expected to be in 0-1 normalized format.\n",
    "        bbox_color: The color for the bounding boxes (default is red).\n",
    "    \"\"\"\n",
    "    fig, ax = plt.subplots(1)\n",
    "    ax.imshow(image,)\n",
    "    \n",
    "    # Image dimensions\n",
    "    img_h, img_w, *_ = image.shape\n",
    "    \n",
    "    # Iterate over bounding boxes and draw them with labels\n",
    "    for bbox, label in zip(bboxes, labels):\n",
    "\n",
    "        # Denormalize bbox coordinates back to pixel values\n",
    "        # xmin, ymin, xmax, ymax = bbox * np.array([img_w, img_h, img_w, img_h])\n",
    "        xmin, ymin, xmax, ymax = bbox\n",
    "        xmin, ymin, xmax, ymax = bbox * np.array([img_w, img_h, img_w, img_h])\n",
    "        width = xmax - xmin\n",
    "        height = ymax - ymin\n",
    "\n",
    "        # xmin, ymin, width, height = bbox\n",
    "        \n",
    "        # Create a rectangle patch\n",
    "        rect = patches.Rectangle((xmin, ymin), width, height, linewidth=1, edgecolor=bbox_color, facecolor='none')\n",
    "        ax.add_patch(rect)\n",
    "        \n",
    "        # Add label text\n",
    "        ax.text(xmin, ymin - 5, label, color=text_color, fontsize=8, bbox=dict(facecolor=bbox_color, alpha=0.5, pad=1))\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import numpy as np\n",
    "# objs, obj_bbox, triples, img_path, img = vg[index]\n",
    "# img = np.array(Image.open(img_path.replace('png', 'jpg')))\n",
    "triples\n",
    "obj_vocab = vg.vocab['object_idx_to_name']\n",
    "pred_vocb = vg.vocab['pred_idx_to_name']\n",
    "print(len(triples), len(objs))\n",
    "for s,p,o in triples:\n",
    "    print(obj_vocab[objs[s]], pred_vocb[p], obj_vocab[objs[o]])\n",
    "    \n",
    "im = np.array(img).transpose(1,2,0)\n",
    "# print(img.shape)\n",
    "labels = [vg.vocab['object_idx_to_name'][o] for o in objs]\n",
    "# print(labels, obj_bbox[-1])\n",
    "visualize_bounding_boxes(im, obj_bbox, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# index = 1000\n",
    "# obj_idxs = np.array(range(vg.data['objects_per_image'][index].item()))\n",
    "\n",
    "# objs = []\n",
    "# obj_idx_mapping = {}\n",
    "# for i, obj_idx in enumerate(obj_idxs):\n",
    "#     objs.append(vg.data['object_names'][index, obj_idx].item())\n",
    "#     obj_idx_mapping[obj_idx] = i\n",
    "\n",
    "# print([obj_vocab[i] for i in objs])\n",
    "\n",
    "# triples = []\n",
    "# print('relationships_per_image: ', vg.data['relationships_per_image'][index].item())\n",
    "# for r_idx in range(vg.data['relationships_per_image'][index].item()):\n",
    "#     s = vg.data['relationship_subjects'][index, r_idx].item()\n",
    "#     p = vg.data['relationship_predicates'][index, r_idx].item()\n",
    "#     o = vg.data['relationship_objects'][index, r_idx].item()\n",
    "#     s = obj_idx_mapping.get(s, None)\n",
    "#     o = obj_idx_mapping.get(o, None)\n",
    "#     if s is not None and o is not None:\n",
    "#         triples.append([s, p, o])\n",
    "\n",
    "# obj_vocab = vg.vocab['object_idx_to_name']\n",
    "# pred_vocb = vg.vocab['pred_idx_to_name']\n",
    "# for s,p,o in triples:\n",
    "#     print(obj_vocab[objs[s]], pred_vocb[p], obj_vocab[objs[o]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.transforms import CenterCrop\n",
    "import torch\n",
    "import numpy as np\n",
    "index = 1000\n",
    "objs, obj_bbox, triples, img_path = vg[index]\n",
    "HH, WW = np.array(vg.data['image_widths'][index]), np.array(vg.data['image_heights'][index])\n",
    "print(HH,WW)\n",
    "out, valid = vg.center_crop_bboxes_with_filter(WW, HH, obj_bbox, 0.01)\n",
    "out = np.array(out).astype(np.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "index = 1000\n",
    "WW, HH = np.array(vg.data['image_widths'][index]), np.array(vg.data['image_heights'][index])\n",
    "from PIL import Image\n",
    "print(WW,HH)\n",
    "# im = Image.open(img_path.replace('png', 'jpg'))\n",
    "im = Image.open(img_path)\n",
    "im"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "import torch\n",
    "WW, HH = np.array(vg.data['image_widths'][index]), np.array(vg.data['image_heights'][index])\n",
    "obj_bbox = torchvision.transforms.CenterCrop(size=(512,512))(torch.tensor(obj_bbox), spatial_size=(WW,HH))\n",
    "obj_bbox.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "obj_idxs = np.array(range(vg.data['objects_per_image'][index].item()))\n",
    "print(obj_idxs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# attributes_per_object = vg.data['attributes_per_object'][index].item()\n",
    "# object_attributes = vg.data['object_attributes'][index].item()\n",
    "# print(attributes_per_object, object_attributes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "im = Image.open(img_path)\n",
    "print([obj_vocab[i] for i in objs])\n",
    "im"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "index=1\n",
    "objs, obj_bbox, triples, img_path = vg[index]\n",
    "obj_vocab = vg.vocab['object_idx_to_name']\n",
    "pred_vocb = vg.vocab['pred_idx_to_name']\n",
    "# for r_idx in range(vg.data['relationships_per_image'][index].item()):\n",
    "#     s = vg.data['relationship_subjects'][index, r_idx].item()\n",
    "#     p = vg.data['relationship_predicates'][index, r_idx].item()\n",
    "#     o = vg.data['relationship_objects'][index, r_idx].item()\n",
    "for s,p,o in triples:\n",
    "    print(obj_vocab[objs[s]], pred_vocb[p], obj_vocab[objs[o]])\n",
    "\n",
    "im = Image.open(img_path)\n",
    "im"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "triples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vg.vocab.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " # get all object idxs\n",
    "import numpy as np\n",
    "import torch\n",
    "import os\n",
    "import PIL\n",
    "index=300\n",
    "raw_path = vg.image_paths[index].decode('utf-8')\n",
    "raw_base_name = os.path.splitext(os.path.basename(raw_path))[0]\n",
    "img_path = os.path.join(vg.image_dir, os.path.basename(raw_base_name)+'.png')\n",
    "print(img_path)\n",
    "WW, HH = np.array(vg.data['image_widths'][index]), np.array(vg.data['image_heights'][index])\n",
    "obj_idxs = np.array(range(vg.data['objects_per_image'][index].item()))\n",
    "n_objs = len(obj_idxs) \n",
    "print(f'n objs in img {n_objs}')\n",
    "is_valid_obj = [True for _ in range(n_objs)]\n",
    "obj_bbox = np.array(vg.data['object_boxes'][index][:n_objs])\n",
    "obj_bbox, is_valid_obj = vg.filter_invalid_bbox(H=HH, W=WW, bbox=obj_bbox, is_valid_bbox=is_valid_obj)\n",
    "obj_bbox = obj_bbox[is_valid_obj]\n",
    "obj_bbox, is_valid_bb = vg.center_crop_bboxes_with_filter(WW, HH, bboxes=obj_bbox) # crop and resize w.r.t to original image size\n",
    "obj_names =  vg.data['object_names'][index][is_valid_bb]\n",
    "\n",
    "# create mapping\n",
    "obj_idxs = obj_idxs[is_valid_bb]\n",
    "n_objs = len(obj_idxs) \n",
    "objs = torch.LongTensor(n_objs).fill_(-1)\n",
    "obj_idx_mapping = {}\n",
    "for i, obj_idx in enumerate(obj_idxs):\n",
    "    objs[i] = vg.data['object_names'][index, obj_idx].item()\n",
    "    obj_idx_mapping[obj_idx] = i\n",
    "\n",
    "triples = []\n",
    "print('relationships_per_image', vg.data['relationships_per_image'][index].item())\n",
    "for r_idx in range(vg.data['relationships_per_image'][index].item()):\n",
    "    s = vg.data['relationship_subjects'][index, r_idx].item()\n",
    "    p = vg.data['relationship_predicates'][index, r_idx].item()\n",
    "    o = vg.data['relationship_objects'][index, r_idx].item()\n",
    "    s = obj_idx_mapping.get(s, None)\n",
    "    o = obj_idx_mapping.get(o, None)\n",
    "    if s is not None and o is not None:\n",
    "        triples.append([s, p, o])\n",
    "\n",
    "triples = torch.LongTensor(triples)\n",
    "objs, obj_bbox, triples, len(objs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import PIL.Image\n",
    "\n",
    "img = np.array(PIL.Image.open(img_path))\n",
    "boxes = obj_bbox\n",
    "labels = [vg.vocab['object_idx_to_name'][o] for o in objs]\n",
    "print(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "objs, boxes, triples = vg[10]\n",
    "print(len(objs), len(boxes), len(triples))\n",
    "objs, boxes, triples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for s,p,o in triples:\n",
    "    s = vg.vocab['object_idx_to_name'][s]\n",
    "    p = vg.vocab['pred_idx_to_name'][p]\n",
    "    o = vg.vocab['object_idx_to_name'][o]\n",
    "    print(f\"{s} -> {p} -> {o}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "path = '/home/rfsm2/rds/hpc-work/datasets/vg/raw_vg/attributes.json'\n",
    "with open(path, 'r') as f:\n",
    "    data = json.load(f) \n",
    "print(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "path =  '/home/rfsm2/rds/hpc-work/datasets/vg/raw_vg/train.h5'\n",
    "with h5py.File(path, 'r') as hdf:\n",
    "    data_fnames = sorted(hdf.keys())\n",
    "    # for k in data_fnames:\n",
    "        # print(k, hdf[k][:1])\n",
    "    # obj_boxes = hdf['object_boxes'][:]\n",
    "    # obj_names = hdf['object_names'][:]\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obj_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_fnames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '/home/rfsm2/rds/hpc-work/datasets/vg/raw_vg/objects.json'\n",
    "with open(path, 'r') as f:\n",
    "    data = json.load(f) \n",
    "print(data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '/home/rfsm2/rds/hpc-work/datasets/vg/raw_vg/scene_graphs.json'\n",
    "with open(path, 'r') as f:\n",
    "    data = json.load(f) \n",
    "print(data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data[0].keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[0]['relationships']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[0]['image_id']\n",
    "data[0]['regions']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
