{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from hig_data.coco2 import COCOStuffGraphPrecomputedDataset\n",
    "from hig_data.utils import DataLoader\n",
    "import torch\n",
    "# new_coco_val2017_hig.h5\n",
    "# aug_coco_train2017_hig.h5\n",
    "path = '/home/rfsm2/rds/hpc-work/datasets/coco/precomputed_coco_val_paths.json'\n",
    "dataset = COCOStuffGraphPrecomputedDataset(path, swapped=False)\n",
    "from hig_data.utils import DataLoader\n",
    "dls = DataLoader(dataset, batch_size=8, subsample=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from training.networks_edm2_hignn_control import Precond\n",
    "import torch\n",
    "precond = Precond(64, 4, gnn_metadata = dataset[0].metadata()).cuda()\n",
    "blank_image = torch.zeros((1, 192, 64, 64)).cuda()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out, new_graph = precond.unet.hignn(blank_image, dataset[0].clone().cuda())\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(iter(dls))\n",
    "batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '/home/rfsm2/rds/hpc-work/datasets/coco/precomputed_coco_val_paths.json'\n",
    "swapped_dataset = COCOStuffGraphPrecomputedDataset(path, swapped=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 3\n",
    "torch.allclose(dataset[3]['class_node'].x, swapped_dataset[3]['class_node'].x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[0]['class_node'].x[0, :10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "path='/home/rfsm2/rds/hpc-work/datasets/coco/aug_coco_train2017_hig-sd.h5'\n",
    "with h5py.File(path, 'r') as hdf:\n",
    "    _data_fnames = list(hdf.keys())\n",
    "    # _data_fnames.remove('statistics')\n",
    "    # raw_ref_image = hdf[_data_fnames[0]]['image'][:]\n",
    "    print('open', _data_fnames[0])\n",
    "# print('Found {} complete datapoint in {}'.format(len(_data_fnames), path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = dataset[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.bounding_boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "import numpy as np\n",
    "def visualize_bounding_boxes(mask: np.ndarray,\n",
    "                             bboxes: np.ndarray,\n",
    "                             labels: list, bbox_color='red', text_color='white'):\n",
    "    \"\"\"\n",
    "    Visualizes bounding boxes on a numpy array image.\n",
    "    \n",
    "    Arguments:\n",
    "        image: 2D NumPy array representing the image.\n",
    "        bboxes: Bounding boxes as an Nx4 numpy array where each row is [xmin, ymin, xmax, ymax].\n",
    "                Coordinates are expected to be in 0-1 normalized format.\n",
    "        bbox_color: The color for the bounding boxes (default is red).\n",
    "    \"\"\"\n",
    "    fig, ax = plt.subplots(1)\n",
    "    ax.imshow(mask,alpha=0.5)\n",
    "    \n",
    "    # Image dimensions\n",
    "    img_h, img_w, *_ = mask.shape\n",
    "    \n",
    "    # Iterate over bounding boxes and draw them with labels\n",
    "    for bbox, label in zip(bboxes, labels):\n",
    "        # Denormalize bbox coordinates back to pixel values\n",
    "        print(bbox)\n",
    "        xmin, ymin, xmax, ymax = bbox * np.array([img_w, img_h, img_w, img_h])\n",
    "        width = xmax - xmin\n",
    "        height = ymax - ymin\n",
    "        \n",
    "        # Create a rectangle patch\n",
    "        rect = patches.Rectangle((xmin, ymin), width, height, linewidth=1, edgecolor=bbox_color, facecolor='none')\n",
    "        ax.add_patch(rect)\n",
    "        \n",
    "        # Add label text\n",
    "        ax.text(xmin, ymin - 5, str(label), color=text_color, fontsize=8, bbox=dict(facecolor=bbox_color, alpha=0.5, pad=1))\n",
    "\n",
    "    plt.show()\n",
    "data = dataset[1011]\n",
    "visualize_bounding_boxes(data.mask.squeeze(), data.bounding_boxes, data.bounding_box_class)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_batch = next(iter(dls))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from training.encoders import StabilityVAEEncoder\n",
    "from hig_data.visualisation import plot_array_images\n",
    "vae = StabilityVAEEncoder(batch_size=8)\n",
    "\n",
    "sampled = vae.encode_latents(graph_batch.image.to('cuda'))\n",
    "sampled_pixels = vae.decode(sampled)\n",
    "sampled_pixels.shape\n",
    "plot_array_images(sampled_pixels.cpu()) # rotating doesn\n",
    "plot_array_images(graph_batch.mask) # rotating doesnt work\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from hig_data.coco2 import CocoStuffGraphDatasetLightweight\n",
    "from hig_data.utils import DataLoader    \n",
    "from training.encoders import StabilityVAEEncoder\n",
    "# /home/rfsm2/rds/hpc-work/datasets/coco/coco_train_paths.json\n",
    "path = '/home/rfsm2/rds/hpc-work/datasets/coco/coco_val_paths.json'\n",
    "dataset = CocoStuffGraphDatasetLightweight(path)\n",
    "from hig_data.utils import DataLoader  \n",
    "dls = DataLoader(dataset, batch_size=8, augmentation=False)\n",
    "data = next(iter(dls))\n",
    "\n",
    "img, mask, label = data\n",
    "label\n",
    "# i=7\n",
    "# visualize_bounding_boxes(mask[i].squeeze(), label[i]['obj_bbox'], label[i]['obj_class'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from generate_images import edm_sampler\n",
    "import os\n",
    "import re\n",
    "import pickle\n",
    "import torch\n",
    "\n",
    "run_dir = '/home/rfsm2/rds/hpc-work/training_runs/edm2_pretrained/pretrained'\n",
    "pattern=r'edm2-img512-s.*(\\d+).pkl'\n",
    "verbose=True\n",
    "fnames = [entry.name for entry in os.scandir(run_dir) if entry.is_file() and re.fullmatch(pattern, entry.name)]\n",
    "print(fnames)\n",
    "pkl_path = os.path.join(run_dir, max(fnames, key=lambda x: float(re.fullmatch(pattern, x).group(1))))\n",
    "\n",
    "if verbose:\n",
    "    print(f'Loading from {pkl_path} ... ', end='', flush=True)\n",
    "with open(pkl_path, 'rb') as f:\n",
    "    data = pickle.load(f, fix_imports=True, encoding=\"bytes\")\n",
    "\n",
    "sample_shape = (8, 4, 64, 64)\n",
    "noise = torch.randn(sample_shape, device='cuda')\n",
    "sampled = edm_sampler(net=data.ema.to('cuda'), noise=noise, graph=None) # sample images from noise and graph batch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hig_data.visualisation import plot_array_images\n",
    "\n",
    "vae = StabilityVAEEncoder(batch_size=8)\n",
    "sampled_pixels = vae.decode(sampled)\n",
    "sampled_pixels.shape\n",
    "plot_array_images(sampled_pixels.cpu()) # rotating doesnt work\n",
    "# plot_array_images(sampled.cpu()) # rotating doesnt work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hig_data.visualisation import plot_array_images\n",
    "iter_dls = iter(dataloader)\n",
    "img, mask, label = next(iter_dls)\n",
    "plot_array_images(img) # rotating doesnt work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "next(iter(dls))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from hig_data.coco import COCOStuffGraphPrecomputedDataset\n",
    "import torch\n",
    "path = '/home/rfsm2/rds/hpc-work/datasets/coco/coco_train2017_hig.h5'\n",
    "coco_graph = COCOStuffGraphPrecomputedDataset(path)\n",
    "data = coco_graph[10]\n",
    "# data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "path = '/home/rfsm2/rds/hpc-work/datasets/vg/raw_vg/attributes.json'\n",
    "with open(path, 'r') as f:\n",
    "    data = json.load(f) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iter_dls = iter(dls)\n",
    "graph_batch = next(iter_dls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hig_data.visualisation import logging_generate_sample_vis, visualise_het_graph_on_image_batch, plot_array_images,convert_latents_to_pixels\n",
    "from training.encoders import StabilityVAEEncoder\n",
    "import torchvision.transforms.functional as F\n",
    "\n",
    "# You can now specify the interpolation mode, for example:\n",
    "# InterpolationMode.BILIN\n",
    "\n",
    "vae = StabilityVAEEncoder(precomputed_latents=True)\n",
    "\n",
    "image = graph_batch.image\n",
    "print(image.shape)\n",
    "# cropped_image = image[:, :, 8:56, 8:56]\n",
    "# cropped_image = torch.nn.functional.interpolate(cropped_image, size=(64,64), mode='bicubic')\n",
    "# flipped_image = image.flip(-1)\n",
    "# rotated_images = torch.stack([F.rotate(img, angle=5, expand=False, interpolation=F.InterpolationMode.BILINEAR) for img in image])\n",
    "# cropped_image = F.center_crop(rotated_images, 32)\n",
    "print(image.shape)\n",
    "decoded_img = data if vae is None else convert_latents_to_pixels(image, vae)\n",
    "print(decoded_img.shape)\n",
    "# plot_array_images(decoded_img)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_array_images(decoded_img) # rotating doesnt work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_array_images(decoded_img) # flipping doesnt work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_array_images(decoded_img) # cropping works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_array_images(decoded_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "plot_array_images(decoded_img.transpose(2,0,1)[np.newaxis,...])\n",
    "plot_array_images(decoded_img2.transpose(2,0,1)[np.newaxis,...])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hig_data.coco import CocoStuffGraphDataset\n",
    "\n",
    "img_path = '/home/rfsm2/rds/hpc-work/datasets/coco/processed_coco/coco_train2017_256-sdxl.zip'\n",
    "img_path_flipped = '/home/rfsm2/rds/hpc-work/datasets/coco/processed_coco/coco_train2017_256-sdxl-f.zip'\n",
    "mask_path = '/home/rfsm2/rds/hpc-work/datasets/coco/processed_coco/coco_train2017_masks_256.zip'\n",
    "label_path = '/home/rfsm2/rds/hpc-work/datasets/coco/processed_coco/coco_train2017_labels.json'\n",
    "caption_path = '/home/rfsm2/rds/hpc-work/datasets/coco/processed_coco/coco_train2017_caption-clip.zip'\n",
    "vocab_path = \"/home/rfsm2/rds/hpc-work/datasets/coco/processed_coco/class_name_clip_latents.json\"\n",
    "dataset = CocoStuffGraphDataset(img_path,\n",
    "                                mask_path,\n",
    "                                labels_path=label_path,\n",
    "                                captions_path=caption_path,\n",
    "                                vocab_path=vocab_path,\n",
    "                                image_path_flipped=img_path_flipped,\n",
    "                                xflip=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset_tool import graphencodecoco2\n",
    "\n",
    "graphencodecoco2(path='/home/rfsm2/rds/hpc-work/datasets/coco/coco_val_paths.json',\n",
    "                 repeats=1, train=False, batch_size=100, dest='/home/rfsm2/rds/hpc-work/datasets/coco/new_coco_val2017_hig.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from hig_data.coco2 import CocoStuffGraphDatasetLightweight\n",
    "from hig_data.utils import DataLoader\n",
    "import torch\n",
    "root = '/home/rfsm2/rds/hpc-work/datasets/coco/coco_val_paths.json'\n",
    "dataset = CocoStuffGraphDatasetLightweight(root)\n",
    "\n",
    "# for i in range(10):\n",
    "    # img,mask,label = dataset[0]\n",
    "    # print(img.shape, mask.shape)\n",
    "dls = DataLoader(dataset, batch_size=8)\n",
    "\n",
    "graph_batch = next(iter(dls))\n",
    "# graph_batch\n",
    "# vae = StabilityVAEEncoder(batch_size=1)\n",
    "\n",
    "# idx = 0\n",
    "# i = 0\n",
    "# graph = dataset[idx]\n",
    "# raw_idx = dataset._raw_idx[idx]\n",
    "# image_filename = dataset._file_name(dataset._all_fnames['image'][raw_idx])\n",
    "# out_fname = f'{image_filename}_{i}'\n",
    "# print(graph.mask.shape)\n",
    "\n",
    "# # convert latent\n",
    "# img_tensor = graph.image.to('cuda')\n",
    "# mean_std = vae.encode_pixels(img_tensor)[0].cpu()\n",
    "# # store mask at latent size\n",
    "\n",
    "# resized_mask = torch.nn.functional.interpolate(graph.mask, size=(dataset.grid_size, dataset.grid_size), mode='nearest').squeeze() # resize mask to match compression\n",
    "\n",
    "# print(mean_std.shape, resized_mask.shape)\n",
    "# from hig_data.utils import DataLoader\n",
    "# dls = DataLoader(dataset, batch_size=2048)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hig_data.visualisation import plot_array_images\n",
    "\n",
    "plot_array_images(dataset[2].image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(dataset)\n",
    "# dataset._xflip[7000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = dataset[0]\n",
    "data = dataset[236560//2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hig_data.visualisation import logging_generate_sample_vis, visualise_het_graph_on_image_batch, plot_array_images,convert_latents_to_pixels\n",
    "from training.encoders import StabilityVAEEncoder\n",
    "vae = StabilityVAEEncoder()\n",
    "data = dataset[1].image\n",
    "data2 = dataset[(236560//2)+1].image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.allclose(data, data2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "decoded_img = data if vae is None else convert_latents_to_pixels(torch.tensor(data), vae)\n",
    "decoded_img2 = data2 if vae is None else convert_latents_to_pixels(torch.tensor(data2), vae)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "plot_array_images(decoded_img.transpose(2,0,1)[np.newaxis,...])\n",
    "plot_array_images(decoded_img2.transpose(2,0,1)[np.newaxis,...])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "data = dataset[1]\n",
    "data2 = dataset[(236560//2)+1]\n",
    "bb_1 = data['instance_to_image'].edge_index\n",
    "bb_2 = data2['instance_to_image'].edge_index\n",
    "from hig_data.coco import linear_to_2d_idx\n",
    "bb_1, bb_2\n",
    "img_bb_1 = linear_to_2d_idx(torch.tensor(bb_1[1]))[:,:200]\n",
    "img_bb_2 = linear_to_2d_idx(torch.tensor(bb_2[1]))[:,:200]\n",
    "\n",
    "from hig_data.visualisation import plot_array_images\n",
    "image = torch.zeros((2, 1, 32, 32))\n",
    "image[0,:,img_bb_1[0], img_bb_1[1]] = 1.\n",
    "image[1,:,img_bb_2[0], img_bb_2[1]] = 1.\n",
    "\n",
    "plot_array_images(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from hig_data.visualisation import plot_array_images\n",
    "image = np.zeros((2,1, 32, 32))\n",
    "image[:,:,8:20, 8:20]=1.\n",
    "print(np.mean(image[0]), np.mean(image[1]))\n",
    "plot_array_images(image)\n",
    "plot_array_images(dataset[0].image[:, :3,:,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.loader import DataLoader as GeoDataLoader\n",
    "dls = GeoDataLoader(dataset, batch_size=32, shuffle=True)\n",
    "graph_batch = next(iter(dls))\n",
    "graph_batch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precond.unet.enc['32x32_block0'].hignn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hig_data.visualisation import logging_generate_sample_vis, visualise_het_graph_on_image_batch, plot_array_images\n",
    "# logging_generate_sample_vis(graph_batch, resize_mask=True)\n",
    "from training.encoders import StabilityVAEEncoder\n",
    "vae = StabilityVAEEncoder()\n",
    "img, decoded = visualise_het_graph_on_image_batch(graph_batch, vae=vae,)\n",
    "print(img.shape)\n",
    "plot_array_images(img)\n",
    "plot_array_images(decoded)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from training.networks_edm2_hignn import Precond\n",
    "precond = Precond(32, 3, label_dim=768, gnn_metadata = data.metadata())\n",
    "\n",
    "images = torch.randn(1, 3, 32, 32)\n",
    "rnd_normal = torch.randn([images.shape[0], 1, 1, 1], device=images.device)\n",
    "sigma = (rnd_normal).exp()\n",
    "\n",
    "data = coco_graph[1]\n",
    "out = precond(images, sigma, data, class_labels=data.caption)\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images = torch.randn(1, 3, 32, 32)\n",
    "rnd_normal = torch.randn([images.shape[0], 1, 1, 1], device=images.device)\n",
    "sigma = (rnd_normal).exp()\n",
    "\n",
    "data = coco_graph[1]\n",
    "out = precond(images, sigma, data, class_labels=data.caption)\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from hig_data.coco_annotations import CocoAnnoations\n",
    "\n",
    "root = '/home/rfsm2/rds/hpc-work/datasets/coco/raw_coco/'\n",
    "img_dir = root+'train2017'\n",
    "instances_json = root+'annotations/instances_train2017.json'\n",
    "stuff_json = root+'annotations/stuff_train2017.json'\n",
    "captions_json = root+'annotations/captions_train2017.json'\n",
    "coco_sg = CocoAnnoations(stuff_json=stuff_json, instances_json=instances_json, captions_json=captions_json)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coco_sg.save_dataset_to_json(root+'coco_train2017_labels.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from training.encoders import CLIPEncoder\n",
    "clip = CLIPEncoder(batch_size=1)\n",
    "text_latents = clip.encode_raw_text(text_data, device='cuda')[0].cpu().numpy().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(coco_sg.vocab['object_idx_to_name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coco_sg.save_vocab_to_json('./coco_2017_class_names.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coco_sg.save_dataset_to_json('/home/rfsm2/rds/hpc-work/datasets/coco/coco_val2017_labels.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ijson\n",
    "root = '/home/rfsm2/rds/hpc-work/datasets/coco/'\n",
    "instances_json = root+'annotations/instances_train2017.json'\n",
    "\n",
    "# def load_json_stream(file_path, prefix, keys):\n",
    "#     out = {}\n",
    "#     with open(file_path, \"rb\") as f:\n",
    "#         for item in ijson.items(f, 'item'):\n",
    "#             out[item[keys[0]]] = []\n",
    "#             for key in keys:\n",
    "#                 out[item[keys[0]]].append[item[key]]\n",
    "#     return out\n",
    "\n",
    "# out_data = load_json_stream(instances_json, 'images', 'file_name')\n",
    "import json\n",
    "with open(instances_json) as file:\n",
    "    my_dict = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import PIL\n",
    "import numpy as np\n",
    "def center_crop_imagenet(image_size: int, arr: np.ndarray):\n",
    "    \"\"\"\n",
    "    Center cropping implementation from ADM.\n",
    "    https://github.com/openai/guided-diffusion/blob/8fb3ad9197f16bbc40620447b2742e13458d2831/guided_diffusion/image_datasets.py#L126\n",
    "    \"\"\"\n",
    "    pil_image = PIL.Image.fromarray(arr)\n",
    "    while min(*pil_image.size) >= 2 * image_size:\n",
    "        new_size = tuple(x // 2 for x in pil_image.size)\n",
    "        assert len(new_size) == 2\n",
    "        pil_image = pil_image.resize(new_size, resample=PIL.Image.Resampling.BOX)\n",
    "\n",
    "    scale = image_size / min(*pil_image.size)\n",
    "    new_size = tuple(round(x * scale) for x in pil_image.size)\n",
    "    assert len(new_size) == 2\n",
    "    pil_image = pil_image.resize(new_size, resample=PIL.Image.Resampling.BICUBIC)\n",
    "\n",
    "    arr = np.array(pil_image)\n",
    "    crop_y = (arr.shape[0] - image_size) // 2\n",
    "    crop_x = (arr.shape[1] - image_size) // 2\n",
    "    return arr[crop_y: crop_y + image_size, crop_x: crop_x + image_size]\n",
    "def nearest_crop(image_size: int, arr: np.ndarray):\n",
    "        \"\"\"\n",
    "        Center cropping for semantic masks using NEAREST resampling to preserve label information.\n",
    "        \"\"\"\n",
    "        pil_image = PIL.Image.fromarray(arr)\n",
    "\n",
    "        # Calculate scale based on the target image size\n",
    "        scale = image_size / min(*pil_image.size)\n",
    "        new_size = tuple(round(x * scale) for x in pil_image.size)\n",
    "        assert len(new_size) == 2\n",
    "\n",
    "        # Resize the image using NEAREST resampling\n",
    "        pil_image = pil_image.resize(new_size, resample=PIL.Image.Resampling.NEAREST)\n",
    "\n",
    "        # Convert back to numpy array and crop the center\n",
    "        arr = np.array(pil_image)\n",
    "        crop_y = (arr.shape[0] - image_size) // 2\n",
    "        crop_x = (arr.shape[1] - image_size) // 2\n",
    "        return arr[crop_y: crop_y + image_size, crop_x: crop_x + image_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "import numpy as np\n",
    "def visualize_bounding_boxes(image: np.ndarray, mask: np.ndarray, bboxes: np.ndarray, labels: list, bbox_color='red', text_color='white'):\n",
    "    \"\"\"\n",
    "    Visualizes bounding boxes on a numpy array image.\n",
    "    \n",
    "    Arguments:\n",
    "        image: 2D NumPy array representing the image.\n",
    "        bboxes: Bounding boxes as an Nx4 numpy array where each row is [xmin, ymin, xmax, ymax].\n",
    "                Coordinates are expected to be in 0-1 normalized format.\n",
    "        bbox_color: The color for the bounding boxes (default is red).\n",
    "    \"\"\"\n",
    "    fig, ax = plt.subplots(1)\n",
    "    ax.imshow(image,)\n",
    "    ax.imshow(mask,alpha=0.5)\n",
    "    \n",
    "    # Image dimensions\n",
    "    img_h, img_w, *_ = image.shape\n",
    "    \n",
    "    # Iterate over bounding boxes and draw them with labels\n",
    "    for bbox, label in zip(bboxes, labels):\n",
    "        # Denormalize bbox coordinates back to pixel values\n",
    "        xmin, ymin, xmax, ymax = bbox * np.array([img_w, img_h, img_w, img_h])\n",
    "        width = xmax - xmin\n",
    "        height = ymax - ymin\n",
    "        \n",
    "        # Create a rectangle patch\n",
    "        rect = patches.Rectangle((xmin, ymin), width, height, linewidth=1, edgecolor=bbox_color, facecolor='none')\n",
    "        ax.add_patch(rect)\n",
    "        \n",
    "        # Add label text\n",
    "        ax.text(xmin, ymin - 5, label, color=text_color, fontsize=8, bbox=dict(facecolor=bbox_color, alpha=0.5, pad=1))\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import PIL\n",
    "import numpy as np\n",
    "m=coco_sg[0]\n",
    "# print(m)\n",
    "img_name = m['filename']\n",
    "# print(m)\n",
    "\n",
    "img = np.array(PIL.Image.open(root+f'val2017/{img_name}.jpg'))\n",
    "mask = np.array(PIL.Image.open(root+f'val2017_masks/{img_name}.png'))\n",
    "img = center_crop_imagenet(256,img)\n",
    "mask = nearest_crop(256,mask)\n",
    "\n",
    "img = img.reshape(*img.shape[:2], -1)\n",
    "mask = mask.reshape(*mask.shape[:2], -1)\n",
    "\n",
    "# PIL.Image.fromarray(img)\n",
    "PIL.Image.fromarray(mask[:,:,0])\n",
    "\n",
    "# # i = 10\n",
    "# # person = m['obj_bbox'][i]\n",
    "print(sorted(m['obj_class_name']))\n",
    "# # print(m['obj_class'])\n",
    "# # print(person)\n",
    "\n",
    "ids_present = [idx+1 for idx in np.unique(mask) if idx !=255]\n",
    "print(sorted([coco_sg.vocab['object_idx_to_name'][i] for i in ids_present]))\n",
    "\n",
    "print(set.union(set(sorted([coco_sg.vocab['object_idx_to_name'][i] for i in ids_present])), set(sorted(m['obj_class_name']))))\n",
    "\n",
    "# # Denormalize bbox coordinates back to pixel values\n",
    "# person_bb = person * np.array([256,]*4)\n",
    "# person_bb = person_bb.astype(int)\n",
    "# print(person_bb)\n",
    "# PIL.Image.fromarray(img[person_bb[1]:person_bb[3],person_bb[0]:person_bb[2], ])\n",
    "# # print(mask.shape)\n",
    "# cut_out_mask = mask[person_bb[1]:person_bb[3], person_bb[0]:person_bb[2]]\n",
    "# print(cut_out_mask.shape)\n",
    "# PIL.Image.fromarray(mask[person_bb[1]:person_bb[3], person_bb[0]:person_bb[2],0])\n",
    "# # print(mask[person_bb[1]+60:person_bb[3]-70, person_bb[0]+20:person_bb[2]-20,0])\n",
    "# print(mask[person_bb[1]:person_bb[3],person_bb[0]:person_bb[2], ])\n",
    "\n",
    "# visualize_bounding_boxes(cropped, mask, m['obj_bbox'], m['obj_class_name'])\n",
    "# PIL.Image.fromarray(cropped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "root = '/home/rfsm2/rds/hpc-work/edm2_datasets/coco/'\n",
    "instances_json = root+'annotations/instances_val2017.json'\n",
    "stuff_json = root+'annotations/stuff_val2017.json'\n",
    "captions_json = root+'annotations/captions_val2017.json'\n",
    "person_key_points_json = root+'annotations/person_keypoints_val2017.json'\n",
    "\n",
    "for json_file in [instances_json, stuff_json, captions_json, person_key_points_json]:\n",
    "    with open(json_file, 'r') as f:\n",
    "        json_file = json.load(f)\n",
    "    print(list(json_file.keys()))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "person_key_points_json = root+'annotations/person_keypoints_val2017.json'\n",
    "with open(person_key_points_json, 'r') as f:\n",
    "    person_key_points = json.load(f)\n",
    "list(person_key_points['annotations'][0].keys())\n",
    "# person_key_points['categories']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "person_key_points['annotations'][0]['keypoints']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stuff_json['annotations']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "captions_json = root+'annotations/captions_val2017.json'\n",
    "with open(captions_json, 'r') as f:\n",
    "    captions_data = json.load(f)\n",
    "\n",
    "print(list(captions_data.keys()))\n",
    "captions_data['annotations']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hig_data.coco import COCOStuffDataset\n",
    "\n",
    "img_path = '/Users/rupertmenneer/Downloads/coco_val2017_256-sd.zip'\n",
    "mask_path = '/Users/rupertmenneer/Downloads/coco_val2017_masks_256.zip'\n",
    "dataset = COCOStuffDataset(img_path, mask_path, 32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hig_data.coco import COCOStuffDataset, CocoStuffGraphDataset\n",
    "import torch\n",
    "img_path = '/Users/rupertmenneer/Downloads/coco_val2017_256-sd.zip'\n",
    "mask_path = '/Users/rupertmenneer/Downloads/coco_val2017_masks_256.zip'\n",
    "coco_graph = CocoStuffGraphDataset(img_path, mask_path, latent_images=True)\n",
    "data = coco_graph[4]\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data.mask)\n",
    "one_hot = torch.nn.functional.one_hot(data.mask, num_classes=256)\n",
    "print(one_hot.shape)\n",
    "torch.argmax(one_hot[:,:,0,0,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.argmax(data['class_node'].x, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['class_node'].label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hig_data.coco import COCOStuffGraphPrecomputedDataset\n",
    "import torch\n",
    "path = '/Users/rupertmenneer/Downloads/coco_graph_train'\n",
    "coco_graph = COCOStuffGraphPrecomputedDataset(path)\n",
    "data = coco_graph[4]\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hig_data.coco import CocoStuffGraphDataset\n",
    "import numpy as np\n",
    "import os\n",
    "def create_graph_dataset_and_export(output_dir='/Users/rupertmenneer/Downloads/coco_graph'):\n",
    "\n",
    "    img_path = '/Users/rupertmenneer/Downloads/coco_val2017_256-sd.zip'\n",
    "    mask_path = '/Users/rupertmenneer/Downloads/coco_val2017_masks_256.zip'\n",
    "    coco_graph = CocoStuffGraphDataset(img_path, mask_path, latent_images=True)\n",
    "\n",
    "    for idx in range(len(coco_graph)):\n",
    "        image, mask = coco_graph.dataset[idx]\n",
    "        graph = coco_graph[idx]\n",
    "\n",
    "        name = coco_graph.dataset._fnames['image'][coco_graph.dataset._raw_idx[idx]]\n",
    "        raw_id = os.path.splitext(os.path.basename(name))[0].split('-')[-1]\n",
    "\n",
    "        np.save(os.path.join(output_dir, f'{raw_id}_image.npy'), image)\n",
    "        np.save(os.path.join(output_dir, f'{raw_id}_mask.npy'), mask)\n",
    "        np.savez(os.path.join(output_dir, f'{raw_id}_graph'),\n",
    "                 class_node=graph['class_node'].x,\n",
    "                 class_pos=graph['class_node'].pos,\n",
    "                 class_edge=graph['class_node', 'class_edge', 'class_node'].edge_index,\n",
    "                 class_to_image=graph['class_node', 'class_to_image', 'image_node'].edge_index,\n",
    "                 )\n",
    "\n",
    "create_graph_dataset_and_export()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from hig_data.coco import COCOStuffDataset, CocoStuffGraphDataset\n",
    "# import torch\n",
    "# img_path = '/Users/rupertmenneer/Downloads/coco_val2017_256-sd.zip'\n",
    "# mask_path = '/Users/rupertmenneer/Downloads/coco_val2017_masks_256.zip'\n",
    "# coco_graph = CocoStuffGraphDataset(img_path, mask_path, latent_images=True)\n",
    "# data = coco_graph[4]\n",
    "# data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "128<<13"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from hig_data.visualisation import convert_latents_to_pixels\n",
    "# import PIL\n",
    "# import numpy as np\n",
    "# print(data.image.shape)\n",
    "# img = convert_latents_to_pixels(data.image)\n",
    "\n",
    "# PIL.Image.fromarray(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.loader import DataLoader as GeoDataLoader\n",
    "dls = GeoDataLoader(coco_graph, batch_size=32, shuffle=True)\n",
    "# graph_batch = next(iter(dls))\n",
    "# graph_batch\n",
    "\n",
    "# graph_batch['image_node'].pos, graph_batch['class_node'].pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_batch = next(iter(dls))\n",
    "graph_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_batch['class_node'].label[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, graph_batch in enumerate(dls):\n",
    "    print(i, graph_batch.image.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_batch.detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_batch['class_node'].label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.zeros((2, 0), dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_batch['image_node'].pos, graph_batch['class_node'].pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from hig_data.visualisation import save_image_batch_list\n",
    "# print(graph_batch.image.shape)\n",
    "# np_img = graph_batch.image[:, :3].cpu().numpy().transpose(0, 2, 3, 1)\n",
    "# save_image_batch_list([np_img,np_img], row_labels=['img', 'img'], sample_batch_size=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hig_data.visualisation import logging_generate_sample_vis, visualise_het_graph_on_image_batch, plot_array_images\n",
    "# logging_generate_sample_vis(graph_batch, resize_mask=True)\n",
    "from training.encoders import StabilityVAEEncoder\n",
    "vae = StabilityVAEEncoder()\n",
    "img, decoded = visualise_het_graph_on_image_batch(graph_batch, vae=vae,)\n",
    "print(img.shape)\n",
    "plot_array_images(img)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_array_images(decoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from training.networks_edm2_hignn import Precond\n",
    "precond = Precond(32, 3, gnn_metadata = data.metadata())\n",
    "\n",
    "images = torch.randn(1, 3, 32, 32)\n",
    "rnd_normal = torch.randn([images.shape[0], 1, 1, 1], device=images.device)\n",
    "sigma = (rnd_normal).exp()\n",
    "\n",
    "data = coco_graph[1]\n",
    "out = precond(images, sigma, data)\n",
    "out.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
